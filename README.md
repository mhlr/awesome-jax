# awesome-jax: List of awesome JAX resources

## The JAX Library
- repo: https://github.com/google/jax
- docs: https://jax.readthedocs.io/en/latest/
- sysml'18 paper: https://research.google/pubs/pub47008/

JAX should be cited by referring to the github repository:
```
    @software{jax2018github,
      author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
      title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
      url = {http://github.com/google/jax},
      version = {0.2.5},
      year = {2018},
    }
```

## Deep Learning libraries 
- Flax: https://github.com/google/flax
- Trax: https://github.com/google/trax
- Haiku: https://github.com/google/trax
See JAX repo & docs for a full list

## Tools & Libraries

- Neural Tangents
  - code: https://github.com/google/neural-tangents
  - paper: https://arxiv.org/abs/1912.02803
- TensorNetwork
  - code: https://github.com/google/TensorNetwork
- JAX Molecular Dynamics
  - code: https://github.com/google/jax-md
- Alibaba Cloud Quantum Development Platform (ACQDP)
  - code: https://github.com/alibaba/acqdp

## Research done with JAX

- Rethinking attention with Performers
  - paper: https://arxiv.org/abs/2009.14794
- Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One
  - paper: https://arxiv.org/abs/1912.03263
- Lagrangean Neural Networks
  - paper: https://arxiv.org/abs/2003.04630
- Finite Versus Infinite Neural Networks: an Empirical Study
  - paper: https://arxiv.org/abs/2007.15801
- Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains
  - paper: https://arxiv.org/abs/2006.10739
- Reformer: The Efficient Transformer
  - paper: https://arxiv.org/abs/2001.04451
- Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
  - paper: https://arxiv.org/abs/1902.06720

## Popular Articles

- [Massively parallel MCMC with JAX](https://rlouf.github.io/post/jax-random-walk-metropolis/)
- [Meta-Learning in 50 Lines of JAX](https://blog.evjang.com/2019/02/maml-jax.html)
- [Normalizing Flows in 100 Lines of JAX](https://blog.evjang.com/2019/07/nf-jax.html)
- [Infinitely Wide Neural-Networks | Neural Tangents Explained](https://towardsdatascience.com/infinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf)
- [Turbocharging SVD with JAX](https://towardsdatascience.com/turbocharging-svd-with-jax-749ae12f93af)
- [Creating Adversarial Examples for Neural Networks with JAX](https://towardsdatascience.com/creating-adversarial-examples-with-jax-from-the-scratch-bf267757f672)
- [JAX: Differentiable Computing by Google](https://towardsdatascience.com/jax-differentiable-computing-by-google-78310859b4ad)
- [Deep Learning with Jax and Elegy: Going beyond TensorFlow, Pytorch, and Keras](https://towardsdatascience.com/deep-learning-with-jax-and-elegy-c0765e3ec31a)
- [Option Greeks in Python: JAX for automatic partial-differentiation of Black-Scholes](https://towardsdatascience.com/option-greeks-in-python-97980df3ab0b)


## (More) Complete Lists of Reverse Dependencies

- wheelodex: https://www.wheelodex.org/projects/jax/rdepends/
- github: https://github.com/google/jax/network/dependents?package_id=UGFja2FnZS0zMjg2NDI1MjA%3D

## Things that should be done with JAX ;)

- Analog cicuit simulator like SPICE
- APL/J/K/FP to XLA compiler

